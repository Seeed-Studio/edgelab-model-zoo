{
    "uuid": "86076c63da1b619e0af9ce0b15dafda5",
    "name": "Gender Detection",
    "version": "1.0.0",
    "category": "Object Detection",
    "algorithm": "Swift-YOLO",
    "description": "The model is a Swift-YOLO model trained on the face gender dataset. It can detect female and male faces in images.",
    "dataset": {
        "name": "Face Gender",
        "url": "https://universe.roboflow.com/seeed-studio-esmjg/person-detection-eetev",
        "download": "https://universe.roboflow.com/ds/JG3NSRibvH?key=lJLGEwG7Zr"
    },
    "network": {
        "batch": 1,
        "input": {
            "type": "image",
            "shape": [
                96,
                96,
                3
            ],
            "remark": "The input image should be resized to 96x96 pixels."
        },
        "output": {
            "type": "bbox",
            "shape": [
                567,
                5
            ],
            "remark": "The output is a 567x5 tensor, where 567 is the number of candidate boxes and 5 is [x, y, w, h, score, [class]]"
        }
    },
    "config": {
        "url": "configs/yolov5/yolov5_tiny_1xb16_300e_coco.py"
    },
    "guidelines": "",
    "license": "MIT",
    "image": "https://files.seeedstudio.com/sscma/static/gender_cls.png",
    "classes": [
        "Female",
        "Male"
    ],
    "benchmark": [
        {
            "backend": "PyTorch",
            "precision": "FLOAT32",
            "metrics": {
                "mAP(%)": 98.2,
                "Flops(M)": 22.6,
                "Params(M)": 0.7
            },
            "url": "https://files.seeedstudio.com/sscma/model_zoo/detection/face/swift-yolo_tiny_gender_96_sha1_9d62ea47febade3f95cde715588b0e98377cd2f5.pth",
            "author": "Seeed Studio",
            "checksum": "md5:e0dd9251cd0f1e4e6fd3bfc081d22769"
        },
        {
            "backend": "ONNX",
            "precision": "FLOAT32",
            "metrics": {
                "mAP(%)": 94.9,
                "Params(M)": 0.7
            },
            "url": "https://files.seeedstudio.com/sscma/model_zoo/detection/face/swift-yolo_tiny_gender_96_float32_sha1_16032922c6531011b1bfdbb2468415211c6dfc85.onnx",
            "author": "Seeed Studio",
            "checksum": "md5:b00096087213bcf63d6126b84989bf62"
        },
        {
            "backend": "TFLite",
            "precision": "FLOAT32",
            "metrics": {
                "mAP(%)": 94.9,
                "Peek RAM(MB)": 1.2
            },
            "url": "https://files.seeedstudio.com/sscma/model_zoo/detection/face/swift-yolo_tiny_gender_96_float32_sha1_dfee634f289c9a7ad692c8bd558bdb3212756a4c.tflite",
            "author": "Seeed Studio",
            "checksum": "md5:f38c4f024609327bd3bf572723f3c755"
        },
        {
            "backend": "TFLite",
            "precision": "INT8",
            "device": [
                "xiao_esp32s3"
            ],
            "metrics": {
                "mAP(%)": 94.9,
                "Peek RAM(MB)": 0.35,
                "Inference(ms)": {
                    "xiao_esp32s3": 200.0
                }
            },
            "url": "https://files.seeedstudio.com/sscma/model_zoo/detection/face/swift-yolo_tiny_gender_96_int8_sha1_8078326f275ce87a371bbb273b010f9dce93f1c0.tflite",
            "author": "Seeed Studio",
            "checksum": "md5:42e591520b7fe34af2abcd27d08910db"
        },
        {
            "backend": "TFLite(vela)",
            "precision": "INT8",
            "device": [
                "grove_vision_ai_we2"
            ],
            "metrics": {
                "mAP(%)": 94.9,
                "Peek RAM(MB)": 0.35,
                "Inference(ms)": {
                    "grove_vision_ai_we2": 20.0
                }
            },
            "url": "https://files.seeedstudio.com/sscma/model_zoo/detection/face/swift-yolo_tiny_gender_96_int8_sha1_8078326f275ce87a371bbb273b010f9dce93f1c0_vela.tflite",
            "author": "Seeed Studio",
            "checksum": "md5:519dadae498ba6f94eb8b7e848d2e37a"
        }
    ],
    "benchmark_note": {
        "Evaluation Parameters": " Confidence Threshold: 0.001, IoU Threshold: 0.55, mAP Eval IoU: 0.50."
    }
}