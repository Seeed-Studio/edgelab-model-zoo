{
    "name": "Face Detection",
    "version": "1.0.0",
    "category": "Object Detection",
    "algorithm": "Swift-YOLO",
    "description": "The model is a Swift-YOLO model trained on the Face dataset. The model can detect faces in images.",
    "dataset": {
        "name": "Face",
        "url": "https://universe.roboflow.com/detection-kgpie/face-detection-j0igc"
    },
    "network": {
        "batch": 1,
        "input": {
            "type": "image",
            "shape": [
                192,
                192,
                3
            ],
            "remark": "The input image should be resized to 192x192 pixels."
        },
        "output": {
            "type": "bbox",
            "shape": [
                2268,
                5
            ],
            "remark": "The output is a 2268x5 tensor, where 2268 is the number of candidate boxes and 5 is [x, y, w, h, score, [class]]"
        }
    },
    "config": {
        "url": "https://github.com/Seeed-Studio/SSCMA/blob/main/configs/yolov5/yolov5_tiny_1xb16_300e_coco.py"
    },
    "guidelines": "",
    "license": "MIT",
    "image": "https://files.seeedstudio.com/sscma/static/detection_face.png",
    "classes": [
        "Face"
    ],
    "benchmark": [
        {
            "backend": "PyTorch",
            "precision": "FLOAT32",
            "metrics": {
                "mAP(%)": 94.4,
                "MACs(MB)": 90.56,
                "Params(MB)": 0.67
            },
            "url": "https://files.seeedstudio.com/sscma/model_zoo/detection/models/yolov5/Face/yolov5_tiny_1xb16_300e_coco_sha1_f2a3f61a271c467748e26f0fd6fdd82d740512ff.pth",
            "author": "Seeed Studio"
        },
        {
            "backend": "ONNX",
            "precision": "FLOAT32",
            "metrics": {
                "mAP(%)": 94.1,
                "Params(MB)": 0.67
            },
            "url": "https://files.seeedstudio.com/sscma/model_zoo/detection/models/yolov5/Face/yolov5_tiny_1xb16_300e_coco_sha1_e530c8df4b4474979cbfe2da447d06ab657289ce.onnx",
            "author": "Seeed Studio"
        },
        {
            "backend": "TFLite",
            "precision": "FLOAT32",
            "metrics": {
                "mAP(%)": 94.1,
                "MACs(MB)": 89.00,
                "Peek RAM(MB)": 1.20
            },
            "url": "https://files.seeedstudio.com/sscma/model_zoo/detection/models/yolov5/Face/yolov5_tiny_1xb16_300e_coco_float32_sha1_a647ee0f7eb8951b3d78c8048159e999029d7051.tflite",
            "author": "Seeed Studio"
        },
        {
            "backend": "TFLite",
            "precision": "INT8",
            "metrics": {
                "mAP(%)": 93.1,
                "MACs(MB)": 89.00,
                "Peek RAM(MB)": 0.35,
                "Inference(ms)": {
                    "xiao_esp32s3": 691.00
                }
            },
            "url": "https://files.seeedstudio.com/sscma/model_zoo/detection/models/yolov5/Face/yolov5_tiny_1xb16_300e_coco_int8_sha1_e707d23e1b45b4a464e9ebedae0f6570a9d35a9c.tflite",
            "author": "Seeed Studio"
        }
    ],
    "benchmark_note": {
        "Evaluation Parameters": " Confidence Threshold: 0.001, IoU Threshold: 0.55, mAP Eval IoU: 0.50."
    }
}